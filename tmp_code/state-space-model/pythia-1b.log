nohup: 忽略输入
[2024-12-15 15:20:47,999] torch.distributed.run: [WARNING] 
[2024-12-15 15:20:47,999] torch.distributed.run: [WARNING] *****************************************
[2024-12-15 15:20:47,999] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-15 15:20:47,999] torch.distributed.run: [WARNING] *****************************************
ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2024-12-15 19:20:57
ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2024-12-15 19:20:57
ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2024-12-15 19:20:57
ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2024-12-15 19:20:58
ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2024-12-15 19:20:58
ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2024-12-15 19:20:58
ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2024-12-15 19:20:58
ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2024-12-15 19:20:58
model:
  model_name_or_path: EleutherAI/pythia-1b
  tokenizer_name_or_path: EleutherAI/pythia-1b
  load_model_state_dict: True
  ckpt_path: /mnt/petrelfs/tangzecheng/local_ckpt/pythia-1b-hf-2048-fromsk-15b/version_1/checkpoints/last.ckpt
  use_custom_module: False
optimizer:
  optimizer_type: adamw
  lr: 0.0001
  beta_1: 0.9
  beta_2: 0.95
  num_training_steps: None
  warmup_steps: None
  peak_lr: 0.0002
  last_lr: 1e-05
lr_scheduler:
  scheduler_type: get_cosine_schedule_with_warmup
  num_training_steps: None
  warmup_steps: None
platform:
  name: pjlab
  hf_model_path: 
  dataset_path: 
  exp_path: /mnt/petrelfs/tangzecheng/local_ckpt
  result_path: 
task:
  task_name: slimpajama
  dataset: [split: train
data_name: slim_pajama
data_path: /mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
module: custom_dataset.slimpajama
processed_data_path: None
dataset_class_name: Slimpajama
max_seq_length: 2048
nworkers: 0
type: hf
batch_size: 18
pin_memory: True
inference_mode: False
cluster_batch: False
require_process: False, split: validation
data_name: slim_pajama
data_path: /mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
module: custom_dataset.slimpajama
processed_data_path: None
dataset_class_name: Slimpajama
max_seq_length: 2048
nworkers: 0
type: hf
batch_size: 1
pin_memory: True
inference_mode: False
cluster_batch: False
require_process: False, split: test
data_name: slim_pajama
data_path: /mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
module: custom_dataset.slimpajama
processed_data_path: None
dataset_class_name: Slimpajama
max_seq_length: 2048
nworkers: 0
type: hf
batch_size: 1
pin_memory: True
inference_mode: True
cluster_batch: False
require_process: False]
  other_cfgs:
    max_generation_length: 48
    testing_max_ctx: 2048
  batch_tokens: 36864
  train_batch_size: 18
experiment:
  experiment_name: pythia-1b-hf-2048-fromsk-15b
  save_top_k: 5
  every_n_train_steps: 400
  train_strategy: deepspeed
  model_module: models.custom_mamba_v3
  block_name: MambaBlock
  version: 1
  state: train
  accumulate_grad_batches: 2
  debug: False
  hf_trainer: False
  low_rank_train: False
  device_num: 8
  node_num: 1
  seed: 42
  max_epochs: 10
  monitor_metric: valid_lm_loss
  max_training_steps: None
  val_check_interval: 400
|||---- 42 ----|||
model:
  model_name_or_path: EleutherAI/pythia-1b
  tokenizer_name_or_path: EleutherAI/pythia-1b
  load_model_state_dict: True
  ckpt_path: /mnt/petrelfs/tangzecheng/local_ckpt/pythia-1b-hf-2048-fromsk-15b/version_1/checkpoints/last.ckpt
  use_custom_module: False
optimizer:
  optimizer_type: adamw
  lr: 0.0001
  beta_1: 0.9
  beta_2: 0.95
  num_training_steps: None
  warmup_steps: None
  peak_lr: 0.0002
  last_lr: 1e-05
lr_scheduler:
  scheduler_type: get_cosine_schedule_with_warmup
  num_training_steps: None
  warmup_steps: None
platform:
  name: pjlab
  hf_model_path: 
  dataset_path: 
  exp_path: /mnt/petrelfs/tangzecheng/local_ckpt
  result_path: 
task:
  task_name: slimpajama
  dataset: [split: train
data_name: slim_pajama
data_path: /mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
module: custom_dataset.slimpajama
processed_data_path: None
dataset_class_name: Slimpajama
max_seq_length: 2048
nworkers: 0
type: hf
batch_size: 18
pin_memory: True
inference_mode: False
cluster_batch: False
require_process: False, split: validation
data_name: slim_pajama
data_path: /mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
module: custom_dataset.slimpajama
processed_data_path: None
dataset_class_name: Slimpajama
max_seq_length: 2048
nworkers: 0
type: hf
batch_size: 1
pin_memory: True
inference_mode: False
cluster_batch: False
require_process: False, split: test
data_name: slim_pajama
data_path: /mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
module: custom_dataset.slimpajama
processed_data_path: None
dataset_class_name: Slimpajama
max_seq_length: 2048
nworkers: 0
type: hf
batch_size: 1
pin_memory: True
inference_mode: True
cluster_batch: False
require_process: False]
  other_cfgs:
    max_generation_length: 48
    testing_max_ctx: 2048
  batch_tokens: 36864
  train_batch_size: 18
experiment:
  experiment_name: pythia-1b-hf-2048-fromsk-15b
  save_top_k: 5
  every_n_train_steps: 400
  train_strategy: deepspeed
  model_module: models.custom_mamba_v3
  block_name: MambaBlock
  version: 1
  state: train
  accumulate_grad_batches: 2
  debug: False
  hf_trainer: False
  low_rank_train: False
  device_num: 8
  node_num: 1
  seed: 42
  max_epochs: 10
  monitor_metric: valid_lm_loss
  max_training_steps: None
  val_check_interval: 400
|||---- 42 ----|||
model:
  model_name_or_path: EleutherAI/pythia-1b
  tokenizer_name_or_path: EleutherAI/pythia-1b
  load_model_state_dict: True
  ckpt_path: /mnt/petrelfs/tangzecheng/local_ckpt/pythia-1b-hf-2048-fromsk-15b/version_1/checkpoints/last.ckpt
  use_custom_module: False
optimizer:
  optimizer_type: adamw
  lr: 0.0001
  beta_1: 0.9
  beta_2: 0.95
  num_training_steps: None
  warmup_steps: None
  peak_lr: 0.0002
  last_lr: 1e-05
lr_scheduler:
  scheduler_type: get_cosine_schedule_with_warmup
  num_training_steps: None
  warmup_steps: None
platform:
  name: pjlab
  hf_model_path: 
  dataset_path: 
  exp_path: /mnt/petrelfs/tangzecheng/local_ckpt
  result_path: 
task:
  task_name: slimpajama
  dataset: [split: train
data_name: slim_pajama
data_path: /mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
module: custom_dataset.slimpajama
processed_data_path: None
dataset_class_name: Slimpajama
max_seq_length: 2048
nworkers: 0
type: hf
batch_size: 18
pin_memory: True
inference_mode: False
cluster_batch: False
require_process: False, split: validation
data_name: slim_pajama
data_path: /mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
module: custom_dataset.slimpajama
processed_data_path: None
dataset_class_name: Slimpajama
max_seq_length: 2048
nworkers: 0
type: hf
batch_size: 1
pin_memory: True
inference_mode: False
cluster_batch: False
require_process: False, split: test
data_name: slim_pajama
data_path: /mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
module: custom_dataset.slimpajama
processed_data_path: None
dataset_class_name: Slimpajama
max_seq_length: 2048
nworkers: 0
type: hf
batch_size: 1
pin_memory: True
inference_mode: True
cluster_batch: False
require_process: False]
  other_cfgs:
    max_generation_length: 48
    testing_max_ctx: 2048
  batch_tokens: 36864
  train_batch_size: 18
experiment:
  experiment_name: pythia-1b-hf-2048-fromsk-15b
  save_top_k: 5
  every_n_train_steps: 400
  train_strategy: deepspeed
  model_module: models.custom_mamba_v3
  block_name: MambaBlock
  version: 1
  state: train
  accumulate_grad_batches: 2
  debug: False
  hf_trainer: False
  low_rank_train: False
  device_num: 8
  node_num: 1
  seed: 42
  max_epochs: 10
  monitor_metric: valid_lm_loss
  max_training_steps: None
  val_check_interval: 400
|||---- 42 ----|||
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
model:
  model_name_or_path: EleutherAI/pythia-1b
  tokenizer_name_or_path: EleutherAI/pythia-1b
  load_model_state_dict: True
  ckpt_path: /mnt/petrelfs/tangzecheng/local_ckpt/pythia-1b-hf-2048-fromsk-15b/version_1/checkpoints/last.ckpt
  use_custom_module: False
optimizer:
  optimizer_type: adamw
  lr: 0.0001
  beta_1: 0.9
  beta_2: 0.95
  num_training_steps: None
  warmup_steps: None
  peak_lr: 0.0002
  last_lr: 1e-05
lr_scheduler:
  scheduler_type: get_cosine_schedule_with_warmup
  num_training_steps: None
  warmup_steps: None
platform:
  name: pjlab
  hf_model_path: 
  dataset_path: 
  exp_path: /mnt/petrelfs/tangzecheng/local_ckpt
  result_path: 
task:
  task_name: slimpajama
  dataset: [split: train
data_name: slim_pajama
data_path: /mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
module: custom_dataset.slimpajama
processed_data_path: None
dataset_class_name: Slimpajama
max_seq_length: 2048
nworkers: 0
type: hf
batch_size: 18
pin_memory: True
inference_mode: False
cluster_batch: False
require_process: False, split: validation
data_name: slim_pajama
data_path: /mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
module: custom_dataset.slimpajama
processed_data_path: None
dataset_class_name: Slimpajama
max_seq_length: 2048
nworkers: 0
type: hf
batch_size: 1
pin_memory: True
inference_mode: False
cluster_batch: False
require_process: False, split: test
data_name: slim_pajama
data_path: /mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
module: custom_dataset.slimpajama
processed_data_path: None
dataset_class_name: Slimpajama
max_seq_length: 2048
nworkers: 0
type: hf
batch_size: 1
pin_memory: True
inference_mode: True
cluster_batch: False
require_process: False]
  other_cfgs:
    max_generation_length: 48
    testing_max_ctx: 2048
  batch_tokens: 36864
  train_batch_size: 18
experiment:
  experiment_name: pythia-1b-hf-2048-fromsk-15b
  save_top_k: 5
  every_n_train_steps: 400
  train_strategy: deepspeed
  model_module: models.custom_mamba_v3
  block_name: MambaBlock
  version: 1
  state: train
  accumulate_grad_batches: 2
  debug: False
  hf_trainer: False
  low_rank_train: False
  device_num: 8
  node_num: 1
  seed: 42
  max_epochs: 10
  monitor_metric: valid_lm_loss
  max_training_steps: None
  val_check_interval: 400
model:
  model_name_or_path: EleutherAI/pythia-1b
  tokenizer_name_or_path: EleutherAI/pythia-1b
  load_model_state_dict: True
  ckpt_path: /mnt/petrelfs/tangzecheng/local_ckpt/pythia-1b-hf-2048-fromsk-15b/version_1/checkpoints/last.ckpt
  use_custom_module: False
optimizer:
  optimizer_type: adamw
  lr: 0.0001
  beta_1: 0.9
  beta_2: 0.95
  num_training_steps: None
  warmup_steps: None
  peak_lr: 0.0002
  last_lr: 1e-05
lr_scheduler:
  scheduler_type: get_cosine_schedule_with_warmup
  num_training_steps: None
  warmup_steps: None
platform:
  name: pjlab
  hf_model_path: 
  dataset_path: 
  exp_path: /mnt/petrelfs/tangzecheng/local_ckpt
  result_path: 
task:
  task_name: slimpajama
  dataset: [split: train
data_name: slim_pajama
data_path: /mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
module: custom_dataset.slimpajama
processed_data_path: None
dataset_class_name: Slimpajama
max_seq_length: 2048
nworkers: 0
type: hf
batch_size: 18
pin_memory: True
inference_mode: False
cluster_batch: False
require_process: False, split: validation
data_name: slim_pajama
data_path: /mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
module: custom_dataset.slimpajama
processed_data_path: None
dataset_class_name: Slimpajama
max_seq_length: 2048
nworkers: 0
type: hf
batch_size: 1
pin_memory: True
inference_mode: False
cluster_batch: False
require_process: False, split: test
data_name: slim_pajama
data_path: /mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
module: custom_dataset.slimpajama
processed_data_path: None
dataset_class_name: Slimpajama
max_seq_length: 2048
nworkers: 0
type: hf
batch_size: 1
pin_memory: True
inference_mode: True
cluster_batch: False
require_process: False]
  other_cfgs:
    max_generation_length: 48
    testing_max_ctx: 2048
  batch_tokens: 36864
  train_batch_size: 18
experiment:
  experiment_name: pythia-1b-hf-2048-fromsk-15b
  save_top_k: 5
  every_n_train_steps: 400
  train_strategy: deepspeed
  model_module: models.custom_mamba_v3
  block_name: MambaBlock
  version: 1
  state: train
  accumulate_grad_batches: 2
  debug: False
  hf_trainer: False
  low_rank_train: False
  device_num: 8
  node_num: 1
  seed: 42
  max_epochs: 10
  monitor_metric: valid_lm_loss
  max_training_steps: None
  val_check_interval: 400
|||---- 42 ----|||
|||---- 42 ----|||
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/mnt/petrelfs/tangzecheng/anaconda3/envs/lte/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/mnt/petrelfs/tangzecheng/anaconda3/envs/lte/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/mnt/petrelfs/tangzecheng/anaconda3/envs/lte/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
model:
  model_name_or_path: EleutherAI/pythia-1b
  tokenizer_name_or_path: EleutherAI/pythia-1b
  load_model_state_dict: True
  ckpt_path: /mnt/petrelfs/tangzecheng/local_ckpt/pythia-1b-hf-2048-fromsk-15b/version_1/checkpoints/last.ckpt
  use_custom_module: False
optimizer:
  optimizer_type: adamw
  lr: 0.0001
  beta_1: 0.9
  beta_2: 0.95
  num_training_steps: None
  warmup_steps: None
  peak_lr: 0.0002
  last_lr: 1e-05
lr_scheduler:
  scheduler_type: get_cosine_schedule_with_warmup
  num_training_steps: None
  warmup_steps: None
platform:
  name: pjlab
  hf_model_path: 
  dataset_path: 
  exp_path: /mnt/petrelfs/tangzecheng/local_ckpt
  result_path: 
task:
  task_name: slimpajama
  dataset: [split: train
data_name: slim_pajama
data_path: /mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
module: custom_dataset.slimpajama
processed_data_path: None
dataset_class_name: Slimpajama
max_seq_length: 2048
nworkers: 0
type: hf
batch_size: 18
pin_memory: True
inference_mode: False
cluster_batch: False
require_process: False, split: validation
data_name: slim_pajama
data_path: /mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
module: custom_dataset.slimpajama
processed_data_path: None
dataset_class_name: Slimpajama
max_seq_length: 2048
nworkers: 0
type: hf
batch_size: 1
pin_memory: True
inference_mode: False
cluster_batch: False
require_process: False, split: test
data_name: slim_pajama
data_path: /mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
module: custom_dataset.slimpajama
processed_data_path: None
dataset_class_name: Slimpajama
max_seq_length: 2048
nworkers: 0
type: hf
batch_size: 1
pin_memory: True
inference_mode: True
cluster_batch: False
require_process: False]
  other_cfgs:
    max_generation_length: 48
    testing_max_ctx: 2048
  batch_tokens: 36864
  train_batch_size: 18
experiment:
  experiment_name: pythia-1b-hf-2048-fromsk-15b
  save_top_k: 5
  every_n_train_steps: 400
  train_strategy: deepspeed
  model_module: models.custom_mamba_v3
  block_name: MambaBlock
  version: 1
  state: train
  accumulate_grad_batches: 2
  debug: False
  hf_trainer: False
  low_rank_train: False
  device_num: 8
  node_num: 1
  seed: 42
  max_epochs: 10
  monitor_metric: valid_lm_loss
  max_training_steps: None
  val_check_interval: 400
|||---- 42 ----|||
model:
  model_name_or_path: EleutherAI/pythia-1b
  tokenizer_name_or_path: EleutherAI/pythia-1b
  load_model_state_dict: True
  ckpt_path: /mnt/petrelfs/tangzecheng/local_ckpt/pythia-1b-hf-2048-fromsk-15b/version_1/checkpoints/last.ckpt
  use_custom_module: False
optimizer:
  optimizer_type: adamw
  lr: 0.0001
  beta_1: 0.9
  beta_2: 0.95
  num_training_steps: None
  warmup_steps: None
  peak_lr: 0.0002
  last_lr: 1e-05
lr_scheduler:
  scheduler_type: get_cosine_schedule_with_warmup
  num_training_steps: None
  warmup_steps: None
platform:
  name: pjlab
  hf_model_path: 
  dataset_path: 
  exp_path: /mnt/petrelfs/tangzecheng/local_ckpt
  result_path: 
task:
  task_name: slimpajama
  dataset: [split: train
data_name: slim_pajama
data_path: /mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
module: custom_dataset.slimpajama
processed_data_path: None
dataset_class_name: Slimpajama
max_seq_length: 2048
nworkers: 0
type: hf
batch_size: 18
pin_memory: True
inference_mode: False
cluster_batch: False
require_process: False, split: validation
data_name: slim_pajama
data_path: /mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
module: custom_dataset.slimpajama
processed_data_path: None
dataset_class_name: Slimpajama
max_seq_length: 2048
nworkers: 0
type: hf
batch_size: 1
pin_memory: True
inference_mode: False
cluster_batch: False
require_process: False, split: test
data_name: slim_pajama
data_path: /mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
module: custom_dataset.slimpajama
processed_data_path: None
dataset_class_name: Slimpajama
max_seq_length: 2048
nworkers: 0
type: hf
batch_size: 1
pin_memory: True
inference_mode: True
cluster_batch: False
require_process: False]
  other_cfgs:
    max_generation_length: 48
    testing_max_ctx: 2048
  batch_tokens: 36864
  train_batch_size: 18
experiment:
  experiment_name: pythia-1b-hf-2048-fromsk-15b
  save_top_k: 5
  every_n_train_steps: 400
  train_strategy: deepspeed
  model_module: models.custom_mamba_v3
  block_name: MambaBlock
  version: 1
  state: train
  accumulate_grad_batches: 2
  debug: False
  hf_trainer: False
  low_rank_train: False
  device_num: 8
  node_num: 1
  seed: 42
  max_epochs: 10
  monitor_metric: valid_lm_loss
  max_training_steps: None
  val_check_interval: 400
model:
  model_name_or_path: EleutherAI/pythia-1b
  tokenizer_name_or_path: EleutherAI/pythia-1b
  load_model_state_dict: True
  ckpt_path: /mnt/petrelfs/tangzecheng/local_ckpt/pythia-1b-hf-2048-fromsk-15b/version_1/checkpoints/last.ckpt
  use_custom_module: False
optimizer:
  optimizer_type: adamw
  lr: 0.0001
  beta_1: 0.9
  beta_2: 0.95
  num_training_steps: None
  warmup_steps: None
  peak_lr: 0.0002
  last_lr: 1e-05
lr_scheduler:
  scheduler_type: get_cosine_schedule_with_warmup
  num_training_steps: None
  warmup_steps: None
platform:
  name: pjlab
  hf_model_path: 
  dataset_path: 
  exp_path: /mnt/petrelfs/tangzecheng/local_ckpt
  result_path: 
task:
  task_name: slimpajama
  dataset: [split: train
data_name: slim_pajama
data_path: /mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
module: custom_dataset.slimpajama
processed_data_path: None
dataset_class_name: Slimpajama
max_seq_length: 2048
nworkers: 0
type: hf
batch_size: 18
pin_memory: True
inference_mode: False
cluster_batch: False
require_process: False, split: validation
data_name: slim_pajama
data_path: /mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
module: custom_dataset.slimpajama
processed_data_path: None
dataset_class_name: Slimpajama
max_seq_length: 2048
nworkers: 0
type: hf
batch_size: 1
pin_memory: True
inference_mode: False
cluster_batch: False
require_process: False, split: test
data_name: slim_pajama
data_path: /mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
module: custom_dataset.slimpajama
processed_data_path: None
dataset_class_name: Slimpajama
max_seq_length: 2048
nworkers: 0
type: hf
batch_size: 1
pin_memory: True
inference_mode: True
cluster_batch: False
require_process: False]
  other_cfgs:
    max_generation_length: 48
    testing_max_ctx: 2048
  batch_tokens: 36864
  train_batch_size: 18
experiment:
  experiment_name: pythia-1b-hf-2048-fromsk-15b
  save_top_k: 5
  every_n_train_steps: 400
  train_strategy: deepspeed
  model_module: models.custom_mamba_v3
  block_name: MambaBlock
  version: 1
  state: train
  accumulate_grad_batches: 2
  debug: False
  hf_trainer: False
  low_rank_train: False
  device_num: 8
  node_num: 1
  seed: 42
  max_epochs: 10
  monitor_metric: valid_lm_loss
  max_training_steps: None
  val_check_interval: 400
|||---- 42 ----|||
|||---- 42 ----|||
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/mnt/petrelfs/tangzecheng/anaconda3/envs/lte/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/lte/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/mnt/petrelfs/tangzecheng/anaconda3/envs/lte/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/mnt/petrelfs/tangzecheng/anaconda3/envs/lte/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/mnt/petrelfs/tangzecheng/anaconda3/envs/lte/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
|||---- loading model state dict from /mnt/petrelfs/tangzecheng/local_ckpt/pythia-1b-hf-2048-fromsk-15b/version_1/checkpoints/last.ckpt ----|||
|||---- loading model state dict from /mnt/petrelfs/tangzecheng/local_ckpt/pythia-1b-hf-2048-fromsk-15b/version_1/checkpoints/last.ckpt ----|||
|||---- loading model state dict from /mnt/petrelfs/tangzecheng/local_ckpt/pythia-1b-hf-2048-fromsk-15b/version_1/checkpoints/last.ckpt ----|||
|||---- loading model state dict from /mnt/petrelfs/tangzecheng/local_ckpt/pythia-1b-hf-2048-fromsk-15b/version_1/checkpoints/last.ckpt ----|||
|||---- loading model state dict from /mnt/petrelfs/tangzecheng/local_ckpt/pythia-1b-hf-2048-fromsk-15b/version_1/checkpoints/last.ckpt ----|||
|||---- loading model state dict from /mnt/petrelfs/tangzecheng/local_ckpt/pythia-1b-hf-2048-fromsk-15b/version_1/checkpoints/last.ckpt ----|||
|||---- loading model state dict from /mnt/petrelfs/tangzecheng/local_ckpt/pythia-1b-hf-2048-fromsk-15b/version_1/checkpoints/last.ckpt ----|||
|||---- loading model state dict from /mnt/petrelfs/tangzecheng/local_ckpt/pythia-1b-hf-2048-fromsk-15b/version_1/checkpoints/last.ckpt ----|||
GPTNeoXForCausalLM(
  (gpt_neox): GPTNeoXModel(
    (embed_in): Embedding(50304, 2048)
    (emb_dropout): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0-15): 16 x GPTNeoXLayer(
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_dropout): Dropout(p=0.0, inplace=False)
        (post_mlp_dropout): Dropout(p=0.0, inplace=False)
        (attention): GPTNeoXAttention(
          (rotary_emb): GPTNeoXRotaryEmbedding()
          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)
          (dense): Linear(in_features=2048, out_features=2048, bias=True)
          (attention_dropout): Dropout(p=0.0, inplace=False)
        )
        (mlp): GPTNeoXMLP(
          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)
          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)
          (act): GELUActivation()
        )
      )
    )
    (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  )
  (embed_out): Linear(in_features=2048, out_features=50304, bias=False)
)
GPTNeoXForCausalLM(
  (gpt_neox): GPTNeoXModel(
    (embed_in): Embedding(50304, 2048)
    (emb_dropout): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0-15): 16 x GPTNeoXLayer(
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_dropout): Dropout(p=0.0, inplace=False)
        (post_mlp_dropout): Dropout(p=0.0, inplace=False)
        (attention): GPTNeoXAttention(
          (rotary_emb): GPTNeoXRotaryEmbedding()
          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)
          (dense): Linear(in_features=2048, out_features=2048, bias=True)
          (attention_dropout): Dropout(p=0.0, inplace=False)
        )
        (mlp): GPTNeoXMLP(
          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)
          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)
          (act): GELUActivation()
        )
      )
    )
    (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  )
  (embed_out): Linear(in_features=2048, out_features=50304, bias=False)
)
/mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
/mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
GPTNeoXForCausalLM(
  (gpt_neox): GPTNeoXModel(
    (embed_in): Embedding(50304, 2048)
    (emb_dropout): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0-15): 16 x GPTNeoXLayer(
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_dropout): Dropout(p=0.0, inplace=False)
        (post_mlp_dropout): Dropout(p=0.0, inplace=False)
        (attention): GPTNeoXAttention(
          (rotary_emb): GPTNeoXRotaryEmbedding()
          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)
          (dense): Linear(in_features=2048, out_features=2048, bias=True)
          (attention_dropout): Dropout(p=0.0, inplace=False)
        )
        (mlp): GPTNeoXMLP(
          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)
          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)
          (act): GELUActivation()
        )
      )
    )
    (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  )
  (embed_out): Linear(in_features=2048, out_features=50304, bias=False)
)
num of train samples: 2740579
/mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
num of train samples: 2740579
/mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
num of valid samples: 1000
|||---- global_batch_size(include grad accmulate): 288 ----|||
|||---- training steps per epoch: 9515 ----|||
|||---- total_training_steps: 95150 ----|||
|||---- using monitor:valid_lm_loss ----|||
|||---- utilize strategy deepspeed ----|||
GPTNeoXForCausalLM(
  (gpt_neox): GPTNeoXModel(
    (embed_in): Embedding(50304, 2048)
    (emb_dropout): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0-15): 16 x GPTNeoXLayer(
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_dropout): Dropout(p=0.0, inplace=False)
        (post_mlp_dropout): Dropout(p=0.0, inplace=False)
        (attention): GPTNeoXAttention(
          (rotary_emb): GPTNeoXRotaryEmbedding()
          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)
          (dense): Linear(in_features=2048, out_features=2048, bias=True)
          (attention_dropout): Dropout(p=0.0, inplace=False)
        )
        (mlp): GPTNeoXMLP(
          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)
          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)
          (act): GELUActivation()
        )
      )
    )
    (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  )
  (embed_out): Linear(in_features=2048, out_features=50304, bias=False)
)
num of valid samples: 1000
|||---- global_batch_size(include grad accmulate): 288 ----|||
|||---- training steps per epoch: 9515 ----|||
|||---- total_training_steps: 95150 ----|||
|||---- using monitor:valid_lm_loss ----|||
|||---- utilize strategy deepspeed ----|||
GPTNeoXForCausalLM(
  (gpt_neox): GPTNeoXModel(
    (embed_in): Embedding(50304, 2048)
    (emb_dropout): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0-15): 16 x GPTNeoXLayer(
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_dropout): Dropout(p=0.0, inplace=False)
        (post_mlp_dropout): Dropout(p=0.0, inplace=False)
        (attention): GPTNeoXAttention(
          (rotary_emb): GPTNeoXRotaryEmbedding()
          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)
          (dense): Linear(in_features=2048, out_features=2048, bias=True)
          (attention_dropout): Dropout(p=0.0, inplace=False)
        )
        (mlp): GPTNeoXMLP(
          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)
          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)
          (act): GELUActivation()
        )
      )
    )
    (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  )
  (embed_out): Linear(in_features=2048, out_features=50304, bias=False)
)
GPTNeoXForCausalLM(
  (gpt_neox): GPTNeoXModel(
    (embed_in): Embedding(50304, 2048)
    (emb_dropout): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0-15): 16 x GPTNeoXLayer(
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_dropout): Dropout(p=0.0, inplace=False)
        (post_mlp_dropout): Dropout(p=0.0, inplace=False)
        (attention): GPTNeoXAttention(
          (rotary_emb): GPTNeoXRotaryEmbedding()
          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)
          (dense): Linear(in_features=2048, out_features=2048, bias=True)
          (attention_dropout): Dropout(p=0.0, inplace=False)
        )
        (mlp): GPTNeoXMLP(
          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)
          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)
          (act): GELUActivation()
        )
      )
    )
    (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  )
  (embed_out): Linear(in_features=2048, out_features=50304, bias=False)
)
[2024-12-15 15:21:24,919] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
[2024-12-15 15:21:24,968] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
GPTNeoXForCausalLM(
  (gpt_neox): GPTNeoXModel(
    (embed_in): Embedding(50304, 2048)
    (emb_dropout): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0-15): 16 x GPTNeoXLayer(
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_dropout): Dropout(p=0.0, inplace=False)
        (post_mlp_dropout): Dropout(p=0.0, inplace=False)
        (attention): GPTNeoXAttention(
          (rotary_emb): GPTNeoXRotaryEmbedding()
          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)
          (dense): Linear(in_features=2048, out_features=2048, bias=True)
          (attention_dropout): Dropout(p=0.0, inplace=False)
        )
        (mlp): GPTNeoXMLP(
          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)
          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)
          (act): GELUActivation()
        )
      )
    )
    (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  )
  (embed_out): Linear(in_features=2048, out_features=50304, bias=False)
)
num of train samples: 2740579
/mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
/mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
/mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
/mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
GPTNeoXForCausalLM(
  (gpt_neox): GPTNeoXModel(
    (embed_in): Embedding(50304, 2048)
    (emb_dropout): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0-15): 16 x GPTNeoXLayer(
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_dropout): Dropout(p=0.0, inplace=False)
        (post_mlp_dropout): Dropout(p=0.0, inplace=False)
        (attention): GPTNeoXAttention(
          (rotary_emb): GPTNeoXRotaryEmbedding()
          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)
          (dense): Linear(in_features=2048, out_features=2048, bias=True)
          (attention_dropout): Dropout(p=0.0, inplace=False)
        )
        (mlp): GPTNeoXMLP(
          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)
          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)
          (act): GELUActivation()
        )
      )
    )
    (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  )
  (embed_out): Linear(in_features=2048, out_features=50304, bias=False)
)
num of valid samples: 1000
|||---- global_batch_size(include grad accmulate): 288 ----|||
|||---- training steps per epoch: 9515 ----|||
|||---- total_training_steps: 95150 ----|||
|||---- using monitor:valid_lm_loss ----|||
|||---- utilize strategy deepspeed ----|||
/mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
[2024-12-15 15:21:25,654] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
num of train samples: 2740579
/mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
num of train samples: 2740579
/mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
num of train samples: 2740579
/mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
/mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
num of train samples: 2740579
/mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
num of valid samples: 1000
|||---- global_batch_size(include grad accmulate): 288 ----|||
|||---- training steps per epoch: 9515 ----|||
|||---- total_training_steps: 95150 ----|||
|||---- using monitor:valid_lm_loss ----|||
|||---- utilize strategy deepspeed ----|||
num of valid samples: 1000
|||---- global_batch_size(include grad accmulate): 288 ----|||num of valid samples: 1000

|||---- training steps per epoch: 9515 ----|||
|||---- total_training_steps: 95150 ----|||
|||---- global_batch_size(include grad accmulate): 288 ----|||
|||---- training steps per epoch: 9515 ----|||
|||---- total_training_steps: 95150 ----|||
|||---- using monitor:valid_lm_loss ----|||
|||---- using monitor:valid_lm_loss ----|||
|||---- utilize strategy deepspeed ----|||
|||---- utilize strategy deepspeed ----|||
[2024-12-15 15:21:26,266] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
num of valid samples: 1000
|||---- global_batch_size(include grad accmulate): 288 ----|||
|||---- training steps per epoch: 9515 ----|||
|||---- total_training_steps: 95150 ----|||
|||---- using monitor:valid_lm_loss ----|||
|||---- utilize strategy deepspeed ----|||
[2024-12-15 15:21:26,525] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-15 15:21:26,525] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-15 15:21:26,559] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
num of train samples: 2740579
/mnt/petrelfs/tangzecheng/local_data/slimpajama-processed/processed_data_2048
num of valid samples: 1000
|||---- global_batch_size(include grad accmulate): 288 ----|||
|||---- training steps per epoch: 9515 ----|||
|||---- total_training_steps: 95150 ----|||
|||---- using monitor:valid_lm_loss ----|||
|||---- utilize strategy deepspeed ----|||
[2024-12-15 15:21:27,008] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8
[2024-12-15 15:21:27,475] [INFO] [comm.py:652:init_distributed] cdb=None
initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8
[2024-12-15 15:21:27,476] [INFO] [comm.py:652:init_distributed] cdb=None
initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8
[2024-12-15 15:21:28,168] [INFO] [comm.py:652:init_distributed] cdb=None
initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8
[2024-12-15 15:21:28,854] [INFO] [comm.py:652:init_distributed] cdb=None
/mnt/petrelfs/tangzecheng/anaconda3/envs/lte/lib/python3.10/site-packages/lightning/fabric/connector.py:571: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   2001 MiB |   4138 MiB |   5879 MiB |   3877 MiB |
|       from large pool |   1993 MiB |   4120 MiB |   5853 MiB |   3860 MiB |
|       from small pool |      8 MiB |     17 MiB |     26 MiB |     17 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   2001 MiB |   4138 MiB |   5879 MiB |   3877 MiB |
|       from large pool |   1993 MiB |   4120 MiB |   5853 MiB |   3860 MiB |
|       from small pool |      8 MiB |     17 MiB |     26 MiB |     17 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   2001 MiB |   4136 MiB |   5877 MiB |   3875 MiB |
|       from large pool |   1993 MiB |   4118 MiB |   5851 MiB |   3858 MiB |
|       from small pool |      8 MiB |     17 MiB |     26 MiB |     17 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   4156 MiB |   4156 MiB |   4156 MiB |      0 B   |
|       from large pool |   4138 MiB |   4138 MiB |   4138 MiB |      0 B   |
|       from small pool |     18 MiB |     18 MiB |     18 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory | 737448 KiB |    916 MiB |   1667 MiB |    946 MiB |
|       from large pool | 728064 KiB |    907 MiB |   1635 MiB |    924 MiB |
|       from small pool |   9384 KiB |      9 MiB |     31 MiB |     22 MiB |
|---------------------------------------------------------------------------|
| Allocations           |     276    |     277    |     536    |     260    |
|       from large pool |      82    |      83    |     148    |      66    |
|       from small pool |     194    |     195    |     388    |     194    |
|---------------------------------------------------------------------------|
| Active allocs         |     276    |     277    |     536    |     260    |
|       from large pool |      82    |      83    |     148    |      66    |
|       from small pool |     194    |     195    |     388    |     194    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      77    |      77    |      77    |       0    |
|       from large pool |      68    |      68    |      68    |       0    |
|       from small pool |       9    |       9    |       9    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      42    |      42    |      83    |      41    |
|       from large pool |      35    |      35    |      65    |      30    |
|       from small pool |       7    |       8    |      18    |      11    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-12-15 15:21:29,288] [INFO] [comm.py:652:init_distributed] cdb=None
initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8
initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8
[2024-12-15 15:21:29,288] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-12-15 15:21:29,288] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-12-15 15:21:29,288] [INFO] [comm.py:652:init_distributed] cdb=None
initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8
[2024-12-15 15:21:29,321] [INFO] [comm.py:652:init_distributed] cdb=None
/mnt/petrelfs/tangzecheng/anaconda3/envs/lte/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /mnt/petrelfs/tangzecheng/local_ckpt/pythia-1b-hf-2048-fromsk-15b/version_1/checkpoints exists and is not empty.
Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
/mnt/petrelfs/tangzecheng/anaconda3/envs/lte/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2024-12-15 15:21:40,125] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
/mnt/petrelfs/tangzecheng/anaconda3/envs/lte/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/lte/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/lte/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2024-12-15 15:21:40,132] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.1, git-hash=unknown, git-branch=unknown
[2024-12-15 15:21:40,133] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
[2024-12-15 15:21:40,136] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
/mnt/petrelfs/tangzecheng/anaconda3/envs/lte/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2024-12-15 15:21:40,137] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
/mnt/petrelfs/tangzecheng/anaconda3/envs/lte/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/lte/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/lte/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2024-12-15 15:21:40,141] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
[2024-12-15 15:21:40,142] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
[2024-12-15 15:21:40,142] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
[2024-12-15 15:21:40,146] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
[2024-12-15 15:21:49,332] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-12-15 15:21:49,351] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-12-15 15:21:49,352] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-12-15 15:21:49,360] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-12-15 15:21:49,360] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'transformers.optimization.AdamW'>
[2024-12-15 15:21:49,360] [WARNING] [engine.py:1240:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****
[2024-12-15 15:21:49,360] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-12-15 15:21:49,360] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-12-15 15:21:49,360] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-12-15 15:21:49,360] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: True
[2024-12-15 15:21:49,360] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-12-15 15:21:52,885] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2024-12-15 15:21:52,886] [INFO] [utils.py:782:see_memory_usage] MA 2.15 GB         Max_MA 2.15 GB         CA 2.41 GB         Max_CA 2 GB 
[2024-12-15 15:21:52,887] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 223.28 GB, percent = 22.2%
[2024-12-15 15:21:53,198] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2024-12-15 15:21:53,199] [INFO] [utils.py:782:see_memory_usage] MA 2.15 GB         Max_MA 2.15 GB         CA 2.41 GB         Max_CA 2 GB 
[2024-12-15 15:21:53,200] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 223.84 GB, percent = 22.2%
[2024-12-15 15:21:53,200] [INFO] [stage_1_and_2.py:544:__init__] optimizer state initialized
[2024-12-15 15:21:53,365] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2024-12-15 15:21:53,366] [INFO] [utils.py:782:see_memory_usage] MA 2.15 GB         Max_MA 2.15 GB         CA 2.41 GB         Max_CA 2 GB 
[2024-12-15 15:21:53,367] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 223.84 GB, percent = 22.2%
[2024-12-15 15:21:53,368] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2024-12-15 15:21:53,369] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-12-15 15:21:53,369] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f1b6c835cc0>
[2024-12-15 15:21:53,369] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2024-12-15 15:21:53,370] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-12-15 15:21:53,370] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": true, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": true, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-12-15 15:21:53,370] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-12-15 15:21:53,370] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-12-15 15:21:53,370] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-12-15 15:21:53,370] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-12-15 15:21:53,370] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-12-15 15:21:53,370] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-12-15 15:21:53,370] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-12-15 15:21:53,370] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-12-15 15:21:53,370] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-12-15 15:21:53,370] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f1b6c835810>
[2024-12-15 15:21:53,370] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-12-15 15:21:53,371] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-12-15 15:21:53,372] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-12-15 15:21:53,372] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-12-15 15:21:53,372] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-12-15 15:21:53,372] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-12-15 15:21:53,372] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-12-15 15:21:53,372] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-12-15 15:21:53,372] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-12-15 15:21:53,372] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-12-15 15:21:53,372] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-12-15 15:21:53,372] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-12-15 15:21:53,372] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-12-15 15:21:53,372] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-12-15 15:21:53,372] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-12-15 15:21:53,372] [INFO] [config.py:1003:print]   steps_per_print .............. None
[2024-12-15 15:21:53,372] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-12-15 15:21:53,372] [INFO] [config.py:1003:print]   train_batch_size ............. 288
[2024-12-15 15:21:53,372] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  18
[2024-12-15 15:21:53,372] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-12-15 15:21:53,372] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-12-15 15:21:53,372] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-12-15 15:21:53,372] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-12-15 15:21:53,372] [INFO] [config.py:1003:print]   world_size ................... 8
[2024-12-15 15:21:53,372] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  True
[2024-12-15 15:21:53,372] [INFO] [config.py:1003:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=PosixPath('/local_nvme'), buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=PosixPath('/local_nvme'), buffer_count=4, pin_memory=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-12-15 15:21:53,372] [INFO] [config.py:1003:print]   zero_enabled ................. True
[2024-12-15 15:21:53,372] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. False
[2024-12-15 15:21:53,372] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 2
[2024-12-15 15:21:53,372] [INFO] [config.py:989:print_user_config]   json = {
    "zero_allow_untested_optimizer": true, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true, 
        "allgather_partitions": true, 
        "reduce_scatter": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "reduce_bucket_size": 2.000000e+08, 
        "sub_group_size": 1.000000e+12, 
        "offload_optimizer": {
            "device": "cpu", 
            "nvme_path": "/local_nvme", 
            "buffer_count": 4, 
            "pin_memory": false
        }, 
        "offload_param": {
            "device": "cpu", 
            "nvme_path": "/local_nvme", 
            "buffer_count": 5, 
            "buffer_size": 1.000000e+08, 
            "max_in_cpu": 1.000000e+09, 
            "pin_memory": false
        }
    }, 
    "activation_checkpointing": {
        "partition_activations": true, 
        "cpu_checkpointing": true, 
        "contiguous_memory_optimization": false, 
        "synchronize_checkpoint_boundary": false
    }, 
    "aio": {
        "block_size": 1.048576e+06, 
        "queue_depth": 8, 
        "single_submit": false, 
        "overlap_events": true, 
        "thread_count": 1
    }, 
    "zero_force_ds_cpu_optimizer": false, 
    "gradient_accumulation_steps": 2, 
    "train_micro_batch_size_per_gpu": 18, 
    "gradient_clipping": 1.0, 
    "bf16": {
        "enabled": true
    }
}
[2024-12-15 15:21:53,372] [INFO] [checkpointing.py:1125:configure] Activation Checkpointing Information
[2024-12-15 15:21:53,372] [INFO] [checkpointing.py:1126:configure] ----Partition Activations True, CPU CHECKPOINTING True
[2024-12-15 15:21:53,373] [INFO] [checkpointing.py:1127:configure] ----contiguous Memory Checkpointing False with False total layers
[2024-12-15 15:21:53,373] [INFO] [checkpointing.py:1128:configure] ----Synchronization False
[2024-12-15 15:21:53,373] [INFO] [checkpointing.py:1129:configure] ----Profiling time in checkpointing False

  | Name     | Type               | Params | Mode 
--------------------------------------------------------
0 | model    | GPTNeoXForCausalLM | 1.0 B  | train
1 | loss_fct | CrossEntropyLoss   | 0      | train
--------------------------------------------------------
1.0 B     Trainable params
0         Non-trainable params
1.0 B     Total params
4,047.127 Total estimated model params size (MB)
232       Modules in train mode
0         Modules in eval mode
Sanity Checking: |          | 0/? [00:00<?, ?it/s]/mnt/petrelfs/tangzecheng/anaconda3/envs/lte/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]                                                                           /mnt/petrelfs/tangzecheng/anaconda3/envs/lte/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/19031 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/19031 [00:00<?, ?it/s] Epoch 0:   0%|          | 1/19031 [00:02<14:48:16,  0.36it/s]Epoch 0:   0%|          | 1/19031 [00:02<14:48:27,  0.36it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   0%|          | 2/19031 [00:08<21:35:52,  0.24it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   0%|          | 2/19031 [00:08<21:36:55,  0.24it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   0%|          | 3/19031 [00:10<17:44:39,  0.30it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   0%|          | 3/19031 [00:10<17:44:43,  0.30it/s, v_num=1, train_lm_loss=3.480]Epoch 0:   0%|          | 4/19031 [00:13<18:08:03,  0.29it/s, v_num=1, train_lm_loss=3.480]Epoch 0:   0%|          | 4/19031 [00:13<18:08:28,  0.29it/s, v_num=1, train_lm_loss=3.650]Epoch 0:   0%|          | 5/19031 [00:15<16:24:58,  0.32it/s, v_num=1, train_lm_loss=3.650]Epoch 0:   0%|          | 5/19031 [00:15<16:25:00,  0.32it/s, v_num=1, train_lm_loss=3.470]Epoch 0:   0%|          | 6/19031 [00:18<16:27:48,  0.32it/s, v_num=1, train_lm_loss=3.470]Epoch 0:   0%|          | 6/19031 [00:18<16:27:50,  0.32it/s, v_num=1, train_lm_loss=3.610]Epoch 0:   0%|          | 7/19031 [00:20<15:30:08,  0.34it/s, v_num=1, train_lm_loss=3.610]Epoch 0:   0%|          | 7/19031 [00:20<15:30:10,  0.34it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   0%|          | 8/19031 [00:24<15:51:35,  0.33it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   0%|          | 8/19031 [00:24<15:51:36,  0.33it/s, v_num=1, train_lm_loss=3.600]Epoch 0:   0%|          | 9/19031 [00:25<15:11:05,  0.35it/s, v_num=1, train_lm_loss=3.600]Epoch 0:   0%|          | 9/19031 [00:25<15:11:08,  0.35it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   0%|          | 10/19031 [00:29<15:25:31,  0.34it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   0%|          | 10/19031 [00:29<15:25:32,  0.34it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   0%|          | 11/19031 [00:31<14:55:09,  0.35it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   0%|          | 11/19031 [00:31<14:55:10,  0.35it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   0%|          | 12/19031 [00:34<15:03:09,  0.35it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   0%|          | 12/19031 [00:34<15:03:10,  0.35it/s, v_num=1, train_lm_loss=3.520]Epoch 0:   0%|          | 13/19031 [00:36<14:38:28,  0.36it/s, v_num=1, train_lm_loss=3.520]Epoch 0:   0%|          | 13/19031 [00:36<14:38:29,  0.36it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   0%|          | 14/19031 [00:39<14:54:24,  0.35it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   0%|          | 14/19031 [00:39<14:54:25,  0.35it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   0%|          | 15/19031 [00:41<14:33:30,  0.36it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   0%|          | 15/19031 [00:41<14:33:31,  0.36it/s, v_num=1, train_lm_loss=3.600]Epoch 0:   0%|          | 16/19031 [00:44<14:41:59,  0.36it/s, v_num=1, train_lm_loss=3.600]Epoch 0:   0%|          | 16/19031 [00:44<14:42:00,  0.36it/s, v_num=1, train_lm_loss=3.450]Epoch 0:   0%|          | 17/19031 [00:46<14:24:10,  0.37it/s, v_num=1, train_lm_loss=3.450]Epoch 0:   0%|          | 17/19031 [00:46<14:24:11,  0.37it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   0%|          | 18/19031 [00:49<14:32:27,  0.36it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   0%|          | 18/19031 [00:49<14:32:27,  0.36it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   0%|          | 19/19031 [00:51<14:16:46,  0.37it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   0%|          | 19/19031 [00:51<14:16:46,  0.37it/s, v_num=1, train_lm_loss=3.500]Epoch 0:   0%|          | 20/19031 [00:54<14:24:27,  0.37it/s, v_num=1, train_lm_loss=3.500]Epoch 0:   0%|          | 20/19031 [00:54<14:24:28,  0.37it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   0%|          | 21/19031 [00:56<14:11:10,  0.37it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   0%|          | 21/19031 [00:56<14:11:11,  0.37it/s, v_num=1, train_lm_loss=3.490]Epoch 0:   0%|          | 22/19031 [00:59<14:14:11,  0.37it/s, v_num=1, train_lm_loss=3.490]Epoch 0:   0%|          | 22/19031 [00:59<14:14:11,  0.37it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   0%|          | 23/19031 [01:01<14:02:03,  0.38it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   0%|          | 23/19031 [01:01<14:02:03,  0.38it/s, v_num=1, train_lm_loss=3.520]Epoch 0:   0%|          | 24/19031 [01:03<14:04:41,  0.38it/s, v_num=1, train_lm_loss=3.520]Epoch 0:   0%|          | 24/19031 [01:03<14:04:41,  0.38it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   0%|          | 25/19031 [01:05<13:53:45,  0.38it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   0%|          | 25/19031 [01:05<13:53:45,  0.38it/s, v_num=1, train_lm_loss=3.620]Epoch 0:   0%|          | 26/19031 [01:08<13:56:28,  0.38it/s, v_num=1, train_lm_loss=3.620]Epoch 0:   0%|          | 26/19031 [01:08<13:56:28,  0.38it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   0%|          | 27/19031 [01:10<13:47:51,  0.38it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   0%|          | 27/19031 [01:10<13:47:51,  0.38it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   0%|          | 28/19031 [01:13<13:51:22,  0.38it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   0%|          | 28/19031 [01:13<13:51:22,  0.38it/s, v_num=1, train_lm_loss=3.440]Epoch 0:   0%|          | 29/19031 [01:15<13:42:26,  0.39it/s, v_num=1, train_lm_loss=3.440]Epoch 0:   0%|          | 29/19031 [01:15<13:42:26,  0.39it/s, v_num=1, train_lm_loss=3.660]Epoch 0:   0%|          | 30/19031 [01:18<13:45:21,  0.38it/s, v_num=1, train_lm_loss=3.660]Epoch 0:   0%|          | 30/19031 [01:18<13:45:22,  0.38it/s, v_num=1, train_lm_loss=3.640]Epoch 0:   0%|          | 31/19031 [01:19<13:37:09,  0.39it/s, v_num=1, train_lm_loss=3.640]Epoch 0:   0%|          | 31/19031 [01:19<13:37:09,  0.39it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   0%|          | 32/19031 [01:22<13:39:31,  0.39it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   0%|          | 32/19031 [01:22<13:39:31,  0.39it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   0%|          | 33/19031 [01:24<13:32:20,  0.39it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   0%|          | 33/19031 [01:24<13:32:20,  0.39it/s, v_num=1, train_lm_loss=3.700]Epoch 0:   0%|          | 34/19031 [01:27<13:35:05,  0.39it/s, v_num=1, train_lm_loss=3.700]Epoch 0:   0%|          | 34/19031 [01:27<13:35:05,  0.39it/s, v_num=1, train_lm_loss=3.490]Epoch 0:   0%|          | 35/19031 [01:29<13:28:11,  0.39it/s, v_num=1, train_lm_loss=3.490]Epoch 0:   0%|          | 35/19031 [01:29<13:28:11,  0.39it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   0%|          | 36/19031 [01:32<13:30:56,  0.39it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   0%|          | 36/19031 [01:32<13:30:56,  0.39it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   0%|          | 37/19031 [01:34<13:24:29,  0.39it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   0%|          | 37/19031 [01:34<13:24:30,  0.39it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   0%|          | 38/19031 [01:36<13:27:17,  0.39it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   0%|          | 38/19031 [01:36<13:27:18,  0.39it/s, v_num=1, train_lm_loss=3.620]Epoch 0:   0%|          | 39/19031 [01:38<13:21:18,  0.40it/s, v_num=1, train_lm_loss=3.620]Epoch 0:   0%|          | 39/19031 [01:38<13:21:18,  0.40it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   0%|          | 40/19031 [01:41<13:24:10,  0.39it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   0%|          | 40/19031 [01:41<13:24:10,  0.39it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   0%|          | 41/19031 [01:43<13:18:34,  0.40it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   0%|          | 41/19031 [01:43<13:18:34,  0.40it/s, v_num=1, train_lm_loss=3.600]Epoch 0:   0%|          | 42/19031 [01:46<13:21:22,  0.39it/s, v_num=1, train_lm_loss=3.600]Epoch 0:   0%|          | 42/19031 [01:46<13:21:22,  0.39it/s, v_num=1, train_lm_loss=3.490]Epoch 0:   0%|          | 43/19031 [01:48<13:16:16,  0.40it/s, v_num=1, train_lm_loss=3.490]Epoch 0:   0%|          | 43/19031 [01:48<13:16:16,  0.40it/s, v_num=1, train_lm_loss=3.520]Epoch 0:   0%|          | 44/19031 [01:51<13:19:09,  0.40it/s, v_num=1, train_lm_loss=3.520]Epoch 0:   0%|          | 44/19031 [01:51<13:19:09,  0.40it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   0%|          | 45/19031 [01:52<13:14:15,  0.40it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   0%|          | 45/19031 [01:52<13:14:15,  0.40it/s, v_num=1, train_lm_loss=3.620]Epoch 0:   0%|          | 46/19031 [01:55<13:17:33,  0.40it/s, v_num=1, train_lm_loss=3.620]Epoch 0:   0%|          | 46/19031 [01:55<13:17:33,  0.40it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   0%|          | 47/19031 [01:57<13:12:55,  0.40it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   0%|          | 47/19031 [01:57<13:12:55,  0.40it/s, v_num=1, train_lm_loss=3.610]Epoch 0:   0%|          | 48/19031 [02:00<13:17:00,  0.40it/s, v_num=1, train_lm_loss=3.610]Epoch 0:   0%|          | 48/19031 [02:00<13:17:00,  0.40it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   0%|          | 49/19031 [02:02<13:13:01,  0.40it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   0%|          | 49/19031 [02:02<13:13:01,  0.40it/s, v_num=1, train_lm_loss=3.610]Epoch 0:   0%|          | 50/19031 [02:05<13:16:42,  0.40it/s, v_num=1, train_lm_loss=3.610]Epoch 0:   0%|          | 50/19031 [02:05<13:16:42,  0.40it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   0%|          | 51/19031 [02:07<13:12:34,  0.40it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   0%|          | 51/19031 [02:07<13:12:34,  0.40it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   0%|          | 52/19031 [02:10<13:15:24,  0.40it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   0%|          | 52/19031 [02:10<13:15:24,  0.40it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   0%|          | 53/19031 [02:12<13:11:11,  0.40it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   0%|          | 53/19031 [02:12<13:11:11,  0.40it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   0%|          | 54/19031 [02:15<13:13:41,  0.40it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   0%|          | 54/19031 [02:15<13:13:42,  0.40it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   0%|          | 55/19031 [02:17<13:09:39,  0.40it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   0%|          | 55/19031 [02:17<13:09:39,  0.40it/s, v_num=1, train_lm_loss=3.640]Epoch 0:   0%|          | 56/19031 [02:20<13:12:15,  0.40it/s, v_num=1, train_lm_loss=3.640]Epoch 0:   0%|          | 56/19031 [02:20<13:12:15,  0.40it/s, v_num=1, train_lm_loss=3.500]Epoch 0:   0%|          | 57/19031 [02:22<13:08:31,  0.40it/s, v_num=1, train_lm_loss=3.500]Epoch 0:   0%|          | 57/19031 [02:22<13:08:32,  0.40it/s, v_num=1, train_lm_loss=3.510]Epoch 0:   0%|          | 58/19031 [02:25<13:12:31,  0.40it/s, v_num=1, train_lm_loss=3.510]Epoch 0:   0%|          | 58/19031 [02:25<13:12:31,  0.40it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   0%|          | 59/19031 [02:27<13:08:55,  0.40it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   0%|          | 59/19031 [02:27<13:08:55,  0.40it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   0%|          | 60/19031 [02:30<13:11:28,  0.40it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   0%|          | 60/19031 [02:30<13:11:28,  0.40it/s, v_num=1, train_lm_loss=3.520]Epoch 0:   0%|          | 61/19031 [02:32<13:07:53,  0.40it/s, v_num=1, train_lm_loss=3.520]Epoch 0:   0%|          | 61/19031 [02:32<13:07:54,  0.40it/s, v_num=1, train_lm_loss=3.620]Epoch 0:   0%|          | 62/19031 [02:34<13:10:09,  0.40it/s, v_num=1, train_lm_loss=3.620]Epoch 0:   0%|          | 62/19031 [02:34<13:10:09,  0.40it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   0%|          | 63/19031 [02:36<13:06:47,  0.40it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   0%|          | 63/19031 [02:36<13:06:47,  0.40it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   0%|          | 64/19031 [02:39<13:08:48,  0.40it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   0%|          | 64/19031 [02:39<13:08:48,  0.40it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   0%|          | 65/19031 [02:41<13:05:30,  0.40it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   0%|          | 65/19031 [02:41<13:05:30,  0.40it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   0%|          | 66/19031 [02:44<13:07:29,  0.40it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   0%|          | 66/19031 [02:44<13:07:29,  0.40it/s, v_num=1, train_lm_loss=3.690]Epoch 0:   0%|          | 67/19031 [02:46<13:04:16,  0.40it/s, v_num=1, train_lm_loss=3.690]Epoch 0:   0%|          | 67/19031 [02:46<13:04:16,  0.40it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   0%|          | 68/19031 [02:49<13:06:29,  0.40it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   0%|          | 68/19031 [02:49<13:06:29,  0.40it/s, v_num=1, train_lm_loss=3.510]Epoch 0:   0%|          | 69/19031 [02:51<13:03:26,  0.40it/s, v_num=1, train_lm_loss=3.510]Epoch 0:   0%|          | 69/19031 [02:51<13:03:26,  0.40it/s, v_num=1, train_lm_loss=3.650]Epoch 0:   0%|          | 70/19031 [02:53<13:05:20,  0.40it/s, v_num=1, train_lm_loss=3.650]Epoch 0:   0%|          | 70/19031 [02:53<13:05:20,  0.40it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   0%|          | 71/19031 [02:55<13:02:29,  0.40it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   0%|          | 71/19031 [02:55<13:02:30,  0.40it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   0%|          | 72/19031 [02:58<13:04:40,  0.40it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   0%|          | 72/19031 [02:58<13:04:40,  0.40it/s, v_num=1, train_lm_loss=3.640]Epoch 0:   0%|          | 73/19031 [03:00<13:01:45,  0.40it/s, v_num=1, train_lm_loss=3.640]Epoch 0:   0%|          | 73/19031 [03:00<13:01:45,  0.40it/s, v_num=1, train_lm_loss=3.610]Epoch 0:   0%|          | 74/19031 [03:03<13:03:46,  0.40it/s, v_num=1, train_lm_loss=3.610]Epoch 0:   0%|          | 74/19031 [03:03<13:03:46,  0.40it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   0%|          | 75/19031 [03:05<13:01:00,  0.40it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   0%|          | 75/19031 [03:05<13:01:00,  0.40it/s, v_num=1, train_lm_loss=3.480]Epoch 0:   0%|          | 76/19031 [03:08<13:02:50,  0.40it/s, v_num=1, train_lm_loss=3.480]Epoch 0:   0%|          | 76/19031 [03:08<13:02:50,  0.40it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   0%|          | 77/19031 [03:10<13:00:06,  0.40it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   0%|          | 77/19031 [03:10<13:00:06,  0.40it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   0%|          | 78/19031 [03:13<13:01:49,  0.40it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   0%|          | 78/19031 [03:13<13:01:49,  0.40it/s, v_num=1, train_lm_loss=3.610]Epoch 0:   0%|          | 79/19031 [03:14<12:59:09,  0.41it/s, v_num=1, train_lm_loss=3.610]Epoch 0:   0%|          | 79/19031 [03:14<12:59:09,  0.41it/s, v_num=1, train_lm_loss=3.440]Epoch 0:   0%|          | 80/19031 [03:18<13:01:53,  0.40it/s, v_num=1, train_lm_loss=3.440]Epoch 0:   0%|          | 80/19031 [03:18<13:01:53,  0.40it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   0%|          | 81/19031 [03:19<12:59:24,  0.41it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   0%|          | 81/19031 [03:19<12:59:24,  0.41it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   0%|          | 82/19031 [03:22<13:00:54,  0.40it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   0%|          | 82/19031 [03:22<13:00:54,  0.40it/s, v_num=1, train_lm_loss=3.600]Epoch 0:   0%|          | 83/19031 [03:24<12:58:20,  0.41it/s, v_num=1, train_lm_loss=3.600]Epoch 0:   0%|          | 83/19031 [03:24<12:58:20,  0.41it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   0%|          | 84/19031 [03:27<12:59:59,  0.40it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   0%|          | 84/19031 [03:27<12:59:59,  0.40it/s, v_num=1, train_lm_loss=3.660]Epoch 0:   0%|          | 85/19031 [03:29<12:57:34,  0.41it/s, v_num=1, train_lm_loss=3.660]Epoch 0:   0%|          | 85/19031 [03:29<12:57:35,  0.41it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   0%|          | 86/19031 [03:32<12:58:59,  0.41it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   0%|          | 86/19031 [03:32<12:58:59,  0.41it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   0%|          | 87/19031 [03:33<12:56:33,  0.41it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   0%|          | 87/19031 [03:33<12:56:33,  0.41it/s, v_num=1, train_lm_loss=3.640]Epoch 0:   0%|          | 88/19031 [03:36<12:58:02,  0.41it/s, v_num=1, train_lm_loss=3.640]Epoch 0:   0%|          | 88/19031 [03:36<12:58:02,  0.41it/s, v_num=1, train_lm_loss=3.650]Epoch 0:   0%|          | 89/19031 [03:38<12:55:41,  0.41it/s, v_num=1, train_lm_loss=3.650]Epoch 0:   0%|          | 89/19031 [03:38<12:55:41,  0.41it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   0%|          | 90/19031 [03:41<12:56:56,  0.41it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   0%|          | 90/19031 [03:41<12:56:56,  0.41it/s, v_num=1, train_lm_loss=3.510]Epoch 0:   0%|          | 91/19031 [03:43<12:54:43,  0.41it/s, v_num=1, train_lm_loss=3.510]Epoch 0:   0%|          | 91/19031 [03:43<12:54:43,  0.41it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   0%|          | 92/19031 [03:46<12:56:06,  0.41it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   0%|          | 92/19031 [03:46<12:56:06,  0.41it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   0%|          | 93/19031 [03:48<12:54:00,  0.41it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   0%|          | 93/19031 [03:48<12:54:00,  0.41it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   0%|          | 94/19031 [03:50<12:55:20,  0.41it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   0%|          | 94/19031 [03:50<12:55:20,  0.41it/s, v_num=1, train_lm_loss=3.480]Epoch 0:   0%|          | 95/19031 [03:52<12:53:11,  0.41it/s, v_num=1, train_lm_loss=3.480]Epoch 0:   0%|          | 95/19031 [03:52<12:53:11,  0.41it/s, v_num=1, train_lm_loss=3.620]Epoch 0:   1%|          | 96/19031 [03:55<12:54:46,  0.41it/s, v_num=1, train_lm_loss=3.620]Epoch 0:   1%|          | 96/19031 [03:55<12:54:46,  0.41it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   1%|          | 97/19031 [03:57<12:52:42,  0.41it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   1%|          | 97/19031 [03:57<12:52:42,  0.41it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   1%|          | 98/19031 [04:00<12:54:11,  0.41it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   1%|          | 98/19031 [04:00<12:54:11,  0.41it/s, v_num=1, train_lm_loss=3.600]Epoch 0:   1%|          | 99/19031 [04:02<12:52:11,  0.41it/s, v_num=1, train_lm_loss=3.600]Epoch 0:   1%|          | 99/19031 [04:02<12:52:11,  0.41it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   1%|          | 100/19031 [04:05<12:53:29,  0.41it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   1%|          | 100/19031 [04:05<12:53:29,  0.41it/s, v_num=1, train_lm_loss=3.500]Epoch 0:   1%|          | 101/19031 [04:06<12:51:27,  0.41it/s, v_num=1, train_lm_loss=3.500]Epoch 0:   1%|          | 101/19031 [04:06<12:51:27,  0.41it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   1%|          | 102/19031 [04:09<12:52:35,  0.41it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   1%|          | 102/19031 [04:09<12:52:35,  0.41it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   1%|          | 103/19031 [04:11<12:50:37,  0.41it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   1%|          | 103/19031 [04:11<12:50:37,  0.41it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   1%|          | 104/19031 [04:14<12:51:50,  0.41it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   1%|          | 104/19031 [04:14<12:51:51,  0.41it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   1%|          | 105/19031 [04:16<12:49:54,  0.41it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   1%|          | 105/19031 [04:16<12:49:54,  0.41it/s, v_num=1, train_lm_loss=3.470]Epoch 0:   1%|          | 106/19031 [04:19<12:51:06,  0.41it/s, v_num=1, train_lm_loss=3.470]Epoch 0:   1%|          | 106/19031 [04:19<12:51:06,  0.41it/s, v_num=1, train_lm_loss=3.620]Epoch 0:   1%|          | 107/19031 [04:20<12:49:14,  0.41it/s, v_num=1, train_lm_loss=3.620]Epoch 0:   1%|          | 107/19031 [04:20<12:49:14,  0.41it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   1%|          | 108/19031 [04:23<12:50:25,  0.41it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   1%|          | 108/19031 [04:23<12:50:25,  0.41it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   1%|          | 109/19031 [04:25<12:48:35,  0.41it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   1%|          | 109/19031 [04:25<12:48:35,  0.41it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   1%|          | 110/19031 [04:28<12:49:50,  0.41it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   1%|          | 110/19031 [04:28<12:49:50,  0.41it/s, v_num=1, train_lm_loss=3.640]Epoch 0:   1%|          | 111/19031 [04:30<12:48:02,  0.41it/s, v_num=1, train_lm_loss=3.640]Epoch 0:   1%|          | 111/19031 [04:30<12:48:02,  0.41it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   1%|          | 112/19031 [04:33<12:49:13,  0.41it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   1%|          | 112/19031 [04:33<12:49:13,  0.41it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   1%|          | 113/19031 [04:35<12:47:27,  0.41it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   1%|          | 113/19031 [04:35<12:47:27,  0.41it/s, v_num=1, train_lm_loss=3.600]Epoch 0:   1%|          | 114/19031 [04:37<12:48:28,  0.41it/s, v_num=1, train_lm_loss=3.600]Epoch 0:   1%|          | 114/19031 [04:37<12:48:28,  0.41it/s, v_num=1, train_lm_loss=3.480]Epoch 0:   1%|          | 115/19031 [04:39<12:46:44,  0.41it/s, v_num=1, train_lm_loss=3.480]Epoch 0:   1%|          | 115/19031 [04:39<12:46:44,  0.41it/s, v_num=1, train_lm_loss=3.620]Epoch 0:   1%|          | 116/19031 [04:42<12:47:53,  0.41it/s, v_num=1, train_lm_loss=3.620]Epoch 0:   1%|          | 116/19031 [04:42<12:47:53,  0.41it/s, v_num=1, train_lm_loss=3.490]Epoch 0:   1%|          | 117/19031 [04:44<12:46:10,  0.41it/s, v_num=1, train_lm_loss=3.490]Epoch 0:   1%|          | 117/19031 [04:44<12:46:10,  0.41it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 118/19031 [04:47<12:47:16,  0.41it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 118/19031 [04:47<12:47:16,  0.41it/s, v_num=1, train_lm_loss=3.600]Epoch 0:   1%|          | 119/19031 [04:49<12:45:36,  0.41it/s, v_num=1, train_lm_loss=3.600]Epoch 0:   1%|          | 119/19031 [04:49<12:45:36,  0.41it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 120/19031 [04:51<12:46:39,  0.41it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 120/19031 [04:51<12:46:40,  0.41it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   1%|          | 121/19031 [04:53<12:45:02,  0.41it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   1%|          | 121/19031 [04:53<12:45:02,  0.41it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 122/19031 [04:56<12:46:07,  0.41it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 122/19031 [04:56<12:46:07,  0.41it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   1%|          | 123/19031 [04:58<12:44:29,  0.41it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   1%|          | 123/19031 [04:58<12:44:29,  0.41it/s, v_num=1, train_lm_loss=3.600]Epoch 0:   1%|          | 124/19031 [05:01<12:45:24,  0.41it/s, v_num=1, train_lm_loss=3.600]Epoch 0:   1%|          | 124/19031 [05:01<12:45:24,  0.41it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   1%|          | 125/19031 [05:03<12:43:49,  0.41it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   1%|          | 125/19031 [05:03<12:43:50,  0.41it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   1%|          | 126/19031 [05:05<12:44:51,  0.41it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   1%|          | 126/19031 [05:05<12:44:51,  0.41it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   1%|          | 127/19031 [05:07<12:43:26,  0.41it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   1%|          | 127/19031 [05:07<12:43:26,  0.41it/s, v_num=1, train_lm_loss=3.650]Epoch 0:   1%|          | 128/19031 [05:10<12:45:10,  0.41it/s, v_num=1, train_lm_loss=3.650]Epoch 0:   1%|          | 128/19031 [05:10<12:45:10,  0.41it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   1%|          | 129/19031 [05:12<12:43:43,  0.41it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   1%|          | 129/19031 [05:12<12:43:43,  0.41it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   1%|          | 130/19031 [05:15<12:45:22,  0.41it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   1%|          | 130/19031 [05:15<12:45:22,  0.41it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 131/19031 [05:17<12:43:55,  0.41it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 131/19031 [05:17<12:43:55,  0.41it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   1%|          | 132/19031 [05:20<12:44:54,  0.41it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   1%|          | 132/19031 [05:20<12:44:54,  0.41it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   1%|          | 133/19031 [05:22<12:43:25,  0.41it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   1%|          | 133/19031 [05:22<12:43:25,  0.41it/s, v_num=1, train_lm_loss=3.490]Epoch 0:   1%|          | 134/19031 [05:25<12:44:29,  0.41it/s, v_num=1, train_lm_loss=3.490]Epoch 0:   1%|          | 134/19031 [05:25<12:44:29,  0.41it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   1%|          | 135/19031 [05:27<12:43:05,  0.41it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   1%|          | 135/19031 [05:27<12:43:05,  0.41it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   1%|          | 136/19031 [05:30<12:44:22,  0.41it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   1%|          | 136/19031 [05:30<12:44:23,  0.41it/s, v_num=1, train_lm_loss=3.510]Epoch 0:   1%|          | 137/19031 [05:31<12:42:55,  0.41it/s, v_num=1, train_lm_loss=3.510]Epoch 0:   1%|          | 137/19031 [05:31<12:42:55,  0.41it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 138/19031 [05:34<12:43:55,  0.41it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 138/19031 [05:34<12:43:55,  0.41it/s, v_num=1, train_lm_loss=3.500]Epoch 0:   1%|          | 139/19031 [05:36<12:42:32,  0.41it/s, v_num=1, train_lm_loss=3.500]Epoch 0:   1%|          | 139/19031 [05:36<12:42:32,  0.41it/s, v_num=1, train_lm_loss=3.620]Epoch 0:   1%|          | 140/19031 [05:39<12:43:28,  0.41it/s, v_num=1, train_lm_loss=3.620]Epoch 0:   1%|          | 140/19031 [05:39<12:43:28,  0.41it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   1%|          | 141/19031 [05:41<12:42:05,  0.41it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   1%|          | 141/19031 [05:41<12:42:05,  0.41it/s, v_num=1, train_lm_loss=3.600]Epoch 0:   1%|          | 142/19031 [05:44<12:42:52,  0.41it/s, v_num=1, train_lm_loss=3.600]Epoch 0:   1%|          | 142/19031 [05:44<12:42:52,  0.41it/s, v_num=1, train_lm_loss=3.500]Epoch 0:   1%|          | 143/19031 [05:45<12:41:32,  0.41it/s, v_num=1, train_lm_loss=3.500]Epoch 0:   1%|          | 143/19031 [05:45<12:41:32,  0.41it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 144/19031 [05:48<12:42:24,  0.41it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 144/19031 [05:48<12:42:24,  0.41it/s, v_num=1, train_lm_loss=3.510]Epoch 0:   1%|          | 145/19031 [05:50<12:41:02,  0.41it/s, v_num=1, train_lm_loss=3.510]Epoch 0:   1%|          | 145/19031 [05:50<12:41:02,  0.41it/s, v_num=1, train_lm_loss=3.660]Epoch 0:   1%|          | 146/19031 [05:53<12:42:00,  0.41it/s, v_num=1, train_lm_loss=3.660]Epoch 0:   1%|          | 146/19031 [05:53<12:42:00,  0.41it/s, v_num=1, train_lm_loss=3.660]Epoch 0:   1%|          | 147/19031 [05:55<12:40:45,  0.41it/s, v_num=1, train_lm_loss=3.660]Epoch 0:   1%|          | 147/19031 [05:55<12:40:45,  0.41it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 148/19031 [05:58<12:41:37,  0.41it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 148/19031 [05:58<12:41:38,  0.41it/s, v_num=1, train_lm_loss=3.520]Epoch 0:   1%|          | 149/19031 [05:59<12:40:19,  0.41it/s, v_num=1, train_lm_loss=3.520]Epoch 0:   1%|          | 149/19031 [05:59<12:40:19,  0.41it/s, v_num=1, train_lm_loss=3.630]Epoch 0:   1%|          | 150/19031 [06:02<12:41:11,  0.41it/s, v_num=1, train_lm_loss=3.630]Epoch 0:   1%|          | 150/19031 [06:02<12:41:11,  0.41it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   1%|          | 151/19031 [06:04<12:39:53,  0.41it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   1%|          | 151/19031 [06:04<12:39:53,  0.41it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   1%|          | 152/19031 [06:07<12:40:41,  0.41it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   1%|          | 152/19031 [06:07<12:40:41,  0.41it/s, v_num=1, train_lm_loss=3.600]Epoch 0:   1%|          | 153/19031 [06:09<12:39:24,  0.41it/s, v_num=1, train_lm_loss=3.600]Epoch 0:   1%|          | 153/19031 [06:09<12:39:24,  0.41it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   1%|          | 154/19031 [06:12<12:40:18,  0.41it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   1%|          | 154/19031 [06:12<12:40:18,  0.41it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   1%|          | 155/19031 [06:13<12:39:03,  0.41it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   1%|          | 155/19031 [06:13<12:39:03,  0.41it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   1%|          | 156/19031 [06:16<12:39:58,  0.41it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   1%|          | 156/19031 [06:16<12:39:58,  0.41it/s, v_num=1, train_lm_loss=3.510]Epoch 0:   1%|          | 157/19031 [06:18<12:38:45,  0.41it/s, v_num=1, train_lm_loss=3.510]Epoch 0:   1%|          | 157/19031 [06:18<12:38:45,  0.41it/s, v_num=1, train_lm_loss=3.510]Epoch 0:   1%|          | 158/19031 [06:21<12:39:33,  0.41it/s, v_num=1, train_lm_loss=3.510]Epoch 0:   1%|          | 158/19031 [06:21<12:39:33,  0.41it/s, v_num=1, train_lm_loss=3.510]Epoch 0:   1%|          | 159/19031 [06:23<12:38:21,  0.41it/s, v_num=1, train_lm_loss=3.510]Epoch 0:   1%|          | 159/19031 [06:23<12:38:22,  0.41it/s, v_num=1, train_lm_loss=3.520]Epoch 0:   1%|          | 160/19031 [06:26<12:39:14,  0.41it/s, v_num=1, train_lm_loss=3.520]Epoch 0:   1%|          | 160/19031 [06:26<12:39:14,  0.41it/s, v_num=1, train_lm_loss=3.430]Epoch 0:   1%|          | 161/19031 [06:28<12:38:01,  0.41it/s, v_num=1, train_lm_loss=3.430]Epoch 0:   1%|          | 161/19031 [06:28<12:38:01,  0.41it/s, v_num=1, train_lm_loss=3.660]Epoch 0:   1%|          | 162/19031 [06:30<12:38:46,  0.41it/s, v_num=1, train_lm_loss=3.660]Epoch 0:   1%|          | 162/19031 [06:30<12:38:46,  0.41it/s, v_num=1, train_lm_loss=3.650]Epoch 0:   1%|          | 163/19031 [06:32<12:37:35,  0.42it/s, v_num=1, train_lm_loss=3.650]Epoch 0:   1%|          | 163/19031 [06:32<12:37:35,  0.42it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 164/19031 [06:35<12:38:59,  0.41it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 164/19031 [06:35<12:38:59,  0.41it/s, v_num=1, train_lm_loss=3.510]Epoch 0:   1%|          | 165/19031 [06:37<12:37:50,  0.41it/s, v_num=1, train_lm_loss=3.510]Epoch 0:   1%|          | 165/19031 [06:37<12:37:50,  0.41it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   1%|          | 166/19031 [06:40<12:39:17,  0.41it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   1%|          | 166/19031 [06:40<12:39:17,  0.41it/s, v_num=1, train_lm_loss=3.460]Epoch 0:   1%|          | 167/19031 [06:42<12:38:08,  0.41it/s, v_num=1, train_lm_loss=3.460]Epoch 0:   1%|          | 167/19031 [06:42<12:38:08,  0.41it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   1%|          | 168/19031 [06:45<12:38:53,  0.41it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   1%|          | 168/19031 [06:45<12:38:53,  0.41it/s, v_num=1, train_lm_loss=3.450]Epoch 0:   1%|          | 169/19031 [06:47<12:37:45,  0.41it/s, v_num=1, train_lm_loss=3.450]Epoch 0:   1%|          | 169/19031 [06:47<12:37:45,  0.41it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   1%|          | 170/19031 [06:50<12:38:34,  0.41it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   1%|          | 170/19031 [06:50<12:38:34,  0.41it/s, v_num=1, train_lm_loss=3.620]Epoch 0:   1%|          | 171/19031 [06:52<12:37:26,  0.41it/s, v_num=1, train_lm_loss=3.620]Epoch 0:   1%|          | 171/19031 [06:52<12:37:26,  0.41it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 172/19031 [06:54<12:38:15,  0.41it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 172/19031 [06:54<12:38:15,  0.41it/s, v_num=1, train_lm_loss=3.440]Epoch 0:   1%|          | 173/19031 [06:56<12:37:07,  0.42it/s, v_num=1, train_lm_loss=3.440]Epoch 0:   1%|          | 173/19031 [06:56<12:37:07,  0.42it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   1%|          | 174/19031 [06:59<12:37:51,  0.41it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   1%|          | 174/19031 [06:59<12:37:51,  0.41it/s, v_num=1, train_lm_loss=3.520]Epoch 0:   1%|          | 175/19031 [07:01<12:36:47,  0.42it/s, v_num=1, train_lm_loss=3.520]Epoch 0:   1%|          | 175/19031 [07:01<12:36:47,  0.42it/s, v_num=1, train_lm_loss=3.510]Epoch 0:   1%|          | 176/19031 [07:04<12:37:28,  0.41it/s, v_num=1, train_lm_loss=3.510]Epoch 0:   1%|          | 176/19031 [07:04<12:37:28,  0.41it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   1%|          | 177/19031 [07:06<12:36:23,  0.42it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   1%|          | 177/19031 [07:06<12:36:23,  0.42it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 178/19031 [07:08<12:37:03,  0.42it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 178/19031 [07:08<12:37:03,  0.42it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   1%|          | 179/19031 [07:10<12:36:01,  0.42it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   1%|          | 179/19031 [07:10<12:36:01,  0.42it/s, v_num=1, train_lm_loss=3.520]Epoch 0:   1%|          | 180/19031 [07:13<12:36:40,  0.42it/s, v_num=1, train_lm_loss=3.520]Epoch 0:   1%|          | 180/19031 [07:13<12:36:40,  0.42it/s, v_num=1, train_lm_loss=3.410]Epoch 0:   1%|          | 181/19031 [07:15<12:35:37,  0.42it/s, v_num=1, train_lm_loss=3.410]Epoch 0:   1%|          | 181/19031 [07:15<12:35:37,  0.42it/s, v_num=1, train_lm_loss=3.470]Epoch 0:   1%|          | 182/19031 [07:18<12:36:18,  0.42it/s, v_num=1, train_lm_loss=3.470]Epoch 0:   1%|          | 182/19031 [07:18<12:36:18,  0.42it/s, v_num=1, train_lm_loss=3.610]Epoch 0:   1%|          | 183/19031 [07:19<12:35:14,  0.42it/s, v_num=1, train_lm_loss=3.610]Epoch 0:   1%|          | 183/19031 [07:19<12:35:14,  0.42it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   1%|          | 184/19031 [07:22<12:35:52,  0.42it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   1%|          | 184/19031 [07:22<12:35:52,  0.42it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   1%|          | 185/19031 [07:24<12:34:49,  0.42it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   1%|          | 185/19031 [07:24<12:34:49,  0.42it/s, v_num=1, train_lm_loss=3.640]Epoch 0:   1%|          | 186/19031 [07:27<12:35:29,  0.42it/s, v_num=1, train_lm_loss=3.640]Epoch 0:   1%|          | 186/19031 [07:27<12:35:29,  0.42it/s, v_num=1, train_lm_loss=3.440]Epoch 0:   1%|          | 187/19031 [07:29<12:34:28,  0.42it/s, v_num=1, train_lm_loss=3.440]Epoch 0:   1%|          | 187/19031 [07:29<12:34:28,  0.42it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   1%|          | 188/19031 [07:32<12:35:09,  0.42it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   1%|          | 188/19031 [07:32<12:35:09,  0.42it/s, v_num=1, train_lm_loss=3.440]Epoch 0:   1%|          | 189/19031 [07:33<12:34:08,  0.42it/s, v_num=1, train_lm_loss=3.440]Epoch 0:   1%|          | 189/19031 [07:33<12:34:08,  0.42it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 190/19031 [07:36<12:34:50,  0.42it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 190/19031 [07:36<12:34:50,  0.42it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   1%|          | 191/19031 [07:38<12:33:50,  0.42it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   1%|          | 191/19031 [07:38<12:33:50,  0.42it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   1%|          | 192/19031 [07:41<12:34:35,  0.42it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   1%|          | 192/19031 [07:41<12:34:35,  0.42it/s, v_num=1, train_lm_loss=3.520]Epoch 0:   1%|          | 193/19031 [07:43<12:33:35,  0.42it/s, v_num=1, train_lm_loss=3.520]Epoch 0:   1%|          | 193/19031 [07:43<12:33:35,  0.42it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   1%|          | 194/19031 [07:46<12:34:14,  0.42it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   1%|          | 194/19031 [07:46<12:34:14,  0.42it/s, v_num=1, train_lm_loss=3.490]Epoch 0:   1%|          | 195/19031 [07:47<12:33:17,  0.42it/s, v_num=1, train_lm_loss=3.490]Epoch 0:   1%|          | 195/19031 [07:47<12:33:17,  0.42it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 196/19031 [07:50<12:33:58,  0.42it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 196/19031 [07:50<12:33:58,  0.42it/s, v_num=1, train_lm_loss=3.490]Epoch 0:   1%|          | 197/19031 [07:52<12:33:00,  0.42it/s, v_num=1, train_lm_loss=3.490]Epoch 0:   1%|          | 197/19031 [07:52<12:33:00,  0.42it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   1%|          | 198/19031 [07:55<12:33:46,  0.42it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   1%|          | 198/19031 [07:55<12:33:46,  0.42it/s, v_num=1, train_lm_loss=3.520]Epoch 0:   1%|          | 199/19031 [07:57<12:32:47,  0.42it/s, v_num=1, train_lm_loss=3.520]Epoch 0:   1%|          | 199/19031 [07:57<12:32:48,  0.42it/s, v_num=1, train_lm_loss=3.610]Epoch 0:   1%|          | 200/19031 [08:00<12:33:25,  0.42it/s, v_num=1, train_lm_loss=3.610]Epoch 0:   1%|          | 200/19031 [08:00<12:33:25,  0.42it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 201/19031 [08:01<12:32:28,  0.42it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 201/19031 [08:01<12:32:28,  0.42it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   1%|          | 202/19031 [08:04<12:33:14,  0.42it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   1%|          | 202/19031 [08:04<12:33:14,  0.42it/s, v_num=1, train_lm_loss=3.480]Epoch 0:   1%|          | 203/19031 [08:06<12:32:18,  0.42it/s, v_num=1, train_lm_loss=3.480]Epoch 0:   1%|          | 203/19031 [08:06<12:32:18,  0.42it/s, v_num=1, train_lm_loss=3.490]Epoch 0:   1%|          | 204/19031 [08:09<12:33:00,  0.42it/s, v_num=1, train_lm_loss=3.490]Epoch 0:   1%|          | 204/19031 [08:09<12:33:00,  0.42it/s, v_num=1, train_lm_loss=3.600]Epoch 0:   1%|          | 205/19031 [08:11<12:32:05,  0.42it/s, v_num=1, train_lm_loss=3.600]Epoch 0:   1%|          | 205/19031 [08:11<12:32:05,  0.42it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   1%|          | 206/19031 [08:14<12:32:40,  0.42it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   1%|          | 206/19031 [08:14<12:32:40,  0.42it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   1%|          | 207/19031 [08:16<12:31:46,  0.42it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   1%|          | 207/19031 [08:16<12:31:46,  0.42it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   1%|          | 208/19031 [08:18<12:32:21,  0.42it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   1%|          | 208/19031 [08:18<12:32:21,  0.42it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   1%|          | 209/19031 [08:20<12:31:29,  0.42it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   1%|          | 209/19031 [08:20<12:31:29,  0.42it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   1%|          | 210/19031 [08:23<12:32:04,  0.42it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   1%|          | 210/19031 [08:23<12:32:04,  0.42it/s, v_num=1, train_lm_loss=3.480]Epoch 0:   1%|          | 211/19031 [08:25<12:31:10,  0.42it/s, v_num=1, train_lm_loss=3.480]Epoch 0:   1%|          | 211/19031 [08:25<12:31:10,  0.42it/s, v_num=1, train_lm_loss=3.640]Epoch 0:   1%|          | 212/19031 [08:28<12:31:45,  0.42it/s, v_num=1, train_lm_loss=3.640]Epoch 0:   1%|          | 212/19031 [08:28<12:31:45,  0.42it/s, v_num=1, train_lm_loss=3.600]Epoch 0:   1%|          | 213/19031 [08:29<12:30:52,  0.42it/s, v_num=1, train_lm_loss=3.600]Epoch 0:   1%|          | 213/19031 [08:29<12:30:52,  0.42it/s, v_num=1, train_lm_loss=3.450]Epoch 0:   1%|          | 214/19031 [08:32<12:31:26,  0.42it/s, v_num=1, train_lm_loss=3.450]Epoch 0:   1%|          | 214/19031 [08:32<12:31:26,  0.42it/s, v_num=1, train_lm_loss=3.480]Epoch 0:   1%|          | 215/19031 [08:34<12:30:33,  0.42it/s, v_num=1, train_lm_loss=3.480]Epoch 0:   1%|          | 215/19031 [08:34<12:30:33,  0.42it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 216/19031 [08:37<12:31:10,  0.42it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 216/19031 [08:37<12:31:10,  0.42it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 217/19031 [08:39<12:30:17,  0.42it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 217/19031 [08:39<12:30:17,  0.42it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   1%|          | 218/19031 [08:42<12:30:58,  0.42it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   1%|          | 218/19031 [08:42<12:30:58,  0.42it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   1%|          | 219/19031 [08:43<12:30:05,  0.42it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   1%|          | 219/19031 [08:43<12:30:05,  0.42it/s, v_num=1, train_lm_loss=3.520]Epoch 0:   1%|          | 220/19031 [08:46<12:30:39,  0.42it/s, v_num=1, train_lm_loss=3.520]Epoch 0:   1%|          | 220/19031 [08:46<12:30:39,  0.42it/s, v_num=1, train_lm_loss=3.490]Epoch 0:   1%|          | 221/19031 [08:48<12:29:47,  0.42it/s, v_num=1, train_lm_loss=3.490]Epoch 0:   1%|          | 221/19031 [08:48<12:29:47,  0.42it/s, v_num=1, train_lm_loss=3.670]Epoch 0:   1%|          | 222/19031 [08:51<12:30:27,  0.42it/s, v_num=1, train_lm_loss=3.670]Epoch 0:   1%|          | 222/19031 [08:51<12:30:27,  0.42it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   1%|          | 223/19031 [08:53<12:29:36,  0.42it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   1%|          | 223/19031 [08:53<12:29:36,  0.42it/s, v_num=1, train_lm_loss=3.620]Epoch 0:   1%|          | 224/19031 [08:56<12:30:16,  0.42it/s, v_num=1, train_lm_loss=3.620]Epoch 0:   1%|          | 224/19031 [08:56<12:30:16,  0.42it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   1%|          | 225/19031 [08:58<12:29:28,  0.42it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   1%|          | 225/19031 [08:58<12:29:28,  0.42it/s, v_num=1, train_lm_loss=3.600]Epoch 0:   1%|          | 226/19031 [09:00<12:30:02,  0.42it/s, v_num=1, train_lm_loss=3.600]Epoch 0:   1%|          | 226/19031 [09:00<12:30:02,  0.42it/s, v_num=1, train_lm_loss=3.500]Epoch 0:   1%|          | 227/19031 [09:02<12:29:13,  0.42it/s, v_num=1, train_lm_loss=3.500]Epoch 0:   1%|          | 227/19031 [09:02<12:29:13,  0.42it/s, v_num=1, train_lm_loss=3.440]Epoch 0:   1%|          | 228/19031 [09:05<12:29:49,  0.42it/s, v_num=1, train_lm_loss=3.440]Epoch 0:   1%|          | 228/19031 [09:05<12:29:49,  0.42it/s, v_num=1, train_lm_loss=3.660]Epoch 0:   1%|          | 229/19031 [09:07<12:28:59,  0.42it/s, v_num=1, train_lm_loss=3.660]Epoch 0:   1%|          | 229/19031 [09:07<12:28:59,  0.42it/s, v_num=1, train_lm_loss=3.520]Epoch 0:   1%|          | 230/19031 [09:10<12:29:37,  0.42it/s, v_num=1, train_lm_loss=3.520]Epoch 0:   1%|          | 230/19031 [09:10<12:29:37,  0.42it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   1%|          | 231/19031 [09:12<12:28:47,  0.42it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   1%|          | 231/19031 [09:12<12:28:47,  0.42it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   1%|          | 232/19031 [09:14<12:29:24,  0.42it/s, v_num=1, train_lm_loss=3.530]Epoch 0:   1%|          | 232/19031 [09:14<12:29:24,  0.42it/s, v_num=1, train_lm_loss=3.490]Epoch 0:   1%|          | 233/19031 [09:16<12:28:34,  0.42it/s, v_num=1, train_lm_loss=3.490]Epoch 0:   1%|          | 233/19031 [09:16<12:28:34,  0.42it/s, v_num=1, train_lm_loss=3.520]Epoch 0:   1%|          | 234/19031 [09:19<12:29:08,  0.42it/s, v_num=1, train_lm_loss=3.520]Epoch 0:   1%|          | 234/19031 [09:19<12:29:08,  0.42it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 235/19031 [09:21<12:28:22,  0.42it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 235/19031 [09:21<12:28:22,  0.42it/s, v_num=1, train_lm_loss=3.500]Epoch 0:   1%|          | 236/19031 [09:24<12:28:54,  0.42it/s, v_num=1, train_lm_loss=3.500]Epoch 0:   1%|          | 236/19031 [09:24<12:28:54,  0.42it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 237/19031 [09:26<12:28:08,  0.42it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|          | 237/19031 [09:26<12:28:08,  0.42it/s, v_num=1, train_lm_loss=3.430]Epoch 0:   1%|▏         | 238/19031 [09:28<12:28:40,  0.42it/s, v_num=1, train_lm_loss=3.430]Epoch 0:   1%|▏         | 238/19031 [09:28<12:28:40,  0.42it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   1%|▏         | 239/19031 [09:30<12:27:53,  0.42it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   1%|▏         | 239/19031 [09:30<12:27:53,  0.42it/s, v_num=1, train_lm_loss=3.450]Epoch 0:   1%|▏         | 240/19031 [09:33<12:28:24,  0.42it/s, v_num=1, train_lm_loss=3.450]Epoch 0:   1%|▏         | 240/19031 [09:33<12:28:24,  0.42it/s, v_num=1, train_lm_loss=3.480]Epoch 0:   1%|▏         | 241/19031 [09:35<12:27:37,  0.42it/s, v_num=1, train_lm_loss=3.480]Epoch 0:   1%|▏         | 241/19031 [09:35<12:27:37,  0.42it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|▏         | 242/19031 [09:38<12:28:08,  0.42it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|▏         | 242/19031 [09:38<12:28:08,  0.42it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   1%|▏         | 243/19031 [09:39<12:27:21,  0.42it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   1%|▏         | 243/19031 [09:39<12:27:21,  0.42it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   1%|▏         | 244/19031 [09:42<12:27:52,  0.42it/s, v_num=1, train_lm_loss=3.570]Epoch 0:   1%|▏         | 244/19031 [09:42<12:27:52,  0.42it/s, v_num=1, train_lm_loss=3.610]Epoch 0:   1%|▏         | 245/19031 [09:44<12:27:06,  0.42it/s, v_num=1, train_lm_loss=3.610]Epoch 0:   1%|▏         | 245/19031 [09:44<12:27:06,  0.42it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   1%|▏         | 246/19031 [09:47<12:27:39,  0.42it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   1%|▏         | 246/19031 [09:47<12:27:39,  0.42it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   1%|▏         | 247/19031 [09:49<12:26:53,  0.42it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   1%|▏         | 247/19031 [09:49<12:26:53,  0.42it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   1%|▏         | 248/19031 [09:52<12:27:26,  0.42it/s, v_num=1, train_lm_loss=3.590]Epoch 0:   1%|▏         | 248/19031 [09:52<12:27:26,  0.42it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   1%|▏         | 249/19031 [09:53<12:26:41,  0.42it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   1%|▏         | 249/19031 [09:53<12:26:41,  0.42it/s, v_num=1, train_lm_loss=3.490]Epoch 0:   1%|▏         | 250/19031 [09:56<12:27:15,  0.42it/s, v_num=1, train_lm_loss=3.490]Epoch 0:   1%|▏         | 250/19031 [09:56<12:27:15,  0.42it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|▏         | 251/19031 [09:58<12:26:30,  0.42it/s, v_num=1, train_lm_loss=3.560]Epoch 0:   1%|▏         | 251/19031 [09:58<12:26:30,  0.42it/s, v_num=1, train_lm_loss=3.640]Epoch 0:   1%|▏         | 252/19031 [10:01<12:27:03,  0.42it/s, v_num=1, train_lm_loss=3.640]Epoch 0:   1%|▏         | 252/19031 [10:01<12:27:03,  0.42it/s, v_num=1, train_lm_loss=3.450]Epoch 0:   1%|▏         | 253/19031 [10:03<12:26:18,  0.42it/s, v_num=1, train_lm_loss=3.450]Epoch 0:   1%|▏         | 253/19031 [10:03<12:26:18,  0.42it/s, v_num=1, train_lm_loss=3.500]Epoch 0:   1%|▏         | 254/19031 [10:06<12:26:48,  0.42it/s, v_num=1, train_lm_loss=3.500]Epoch 0:   1%|▏         | 254/19031 [10:06<12:26:48,  0.42it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   1%|▏         | 255/19031 [10:07<12:26:04,  0.42it/s, v_num=1, train_lm_loss=3.540]Epoch 0:   1%|▏         | 255/19031 [10:07<12:26:04,  0.42it/s, v_num=1, train_lm_loss=3.490]Epoch 0:   1%|▏         | 256/19031 [10:10<12:26:35,  0.42it/s, v_num=1, train_lm_loss=3.490]Epoch 0:   1%|▏         | 256/19031 [10:10<12:26:36,  0.42it/s, v_num=1, train_lm_loss=3.620]Epoch 0:   1%|▏         | 257/19031 [10:12<12:25:52,  0.42it/s, v_num=1, train_lm_loss=3.620]Epoch 0:   1%|▏         | 257/19031 [10:12<12:25:52,  0.42it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   1%|▏         | 258/19031 [10:15<12:26:21,  0.42it/s, v_num=1, train_lm_loss=3.580]Epoch 0:   1%|▏         | 258/19031 [10:15<12:26:21,  0.42it/s, v_num=1, train_lm_loss=3.420]Epoch 0:   1%|▏         | 259/19031 [10:17<12:25:36,  0.42it/s, v_num=1, train_lm_loss=3.420]Epoch 0:   1%|▏         | 259/19031 [10:17<12:25:36,  0.42it/s, v_num=1, train_lm_loss=3.340]Epoch 0:   1%|▏         | 260/19031 [10:20<12:26:06,  0.42it/s, v_num=1, train_lm_loss=3.340]Epoch 0:   1%|▏         | 260/19031 [10:20<12:26:06,  0.42it/s, v_num=1, train_lm_loss=3.500]Epoch 0:   1%|▏         | 261/19031 [10:21<12:25:22,  0.42it/s, v_num=1, train_lm_loss=3.500]Epoch 0:   1%|▏         | 261/19031 [10:21<12:25:22,  0.42it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   1%|▏         | 262/19031 [10:24<12:25:56,  0.42it/s, v_num=1, train_lm_loss=3.550]Epoch 0:   1%|▏         | 262/19031 [10:24<12:25:56,  0.42it/s, v_num=1, train_lm_loss=3.460]Epoch 0:   1%|▏         | 263/19031 [10:26<12:25:13,  0.42it/s, v_num=1, train_lm_loss=3.460]Epoch 0:   1%|▏         | 263/19031 [10:26<12:25:13,  0.42it/s, v_num=1, train_lm_loss=3.490]